{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e806bae-0304-4663-9666-47adcdf857e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e99d70f0-0a76-4dd7-b0bf-6b24dc8682ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  140,    94, 16843,   140,   111, 25443,   112, 22177, 40623, 12466,\n",
       "           123, 21169, 16843, 31583, 21169, 16142, 21727, 22177, 45035,   140,\n",
       "           117, 12466,   112, 16843, 22177, 45367,    11, 12466,   123, 15166,\n",
       "         20375, 25443,   120, 35072,   220,   141,   229, 20375, 15166, 12466,\n",
       "           116, 12466,   121, 16843, 12466,   110, 12466,   120, 16843, 21169,\n",
       "           140,   123, 25443,   110, 16142, 22177, 18849, 16843,   220, 21727,\n",
       "         20375, 21169, 15166, 18849, 12466,   118, 15166, 21169, 43108, 16142,\n",
       "         20375, 45367, 12466,   122,   140,   109, 21169, 25443,   111, 15166,\n",
       "           220, 20375, 16843, 30143, 16843, 20375, 12466,   111, 21169, 35072,\n",
       "           140,   114, 16843, 43108, 12466,   109, 45035, 30143, 18849,   220]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bba7349c-81c0-4297-9523-d2e47fe22617",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "input_text = \"я сегодня иду в школу\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729f4c5-0cdb-4b4a-b4d3-a720352bce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация текста\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "# Декодирование и вывод результата\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d99969-5539-45ec-b6d8-955a33447024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Сегодня прекрасный день, потому что и не в мерпование строи кормать оброго телет гружем были '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c32c526-f866-4bdc-a352-6a5f6f5b4b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Александр\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tok = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d07d41-114c-41f3-925f-82daa2ed1ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  140,    94, 16843,   140,   111, 25443,   112, 22177, 40623, 12466,\n",
       "           123, 21169, 16843, 31583, 21169, 16142, 21727, 22177, 45035,   140,\n",
       "           117, 12466,   112, 16843, 22177, 45367,    11, 12466,   123, 15166,\n",
       "         20375, 25443,   120, 35072,   220,   141,   229, 20375, 15166]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "97f39c56-5601-4069-becb-24c98d4069b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tensor([[[-73.5670, -71.8578, -78.1044,  ..., -85.1023, -81.5467, -73.6291],\n",
      "         [-73.5670, -71.8578, -78.1044,  ..., -85.1023, -81.5467, -73.6291],\n",
      "         [-73.5670, -71.8578, -78.1044,  ..., -85.1024, -81.5467, -73.6291],\n",
      "         ...,\n",
      "         [-73.5423, -71.8281, -78.0751,  ..., -85.0805, -81.5228, -73.6000],\n",
      "         [-73.5423, -71.8281, -78.0751,  ..., -85.0805, -81.5228, -73.5999],\n",
      "         [-73.5423, -71.8281, -78.0751,  ..., -85.0805, -81.5228, -73.5999]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GLAAttention(nn.Module):\n",
    "    def __init__(self, Q, K, V, hidden_dim=768, c=5):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.C = c\n",
    "        self.Q = nn.Parameter(Q)\n",
    "        self.K = nn.Parameter(K)\n",
    "        self.V = nn.Parameter(V)\n",
    "        self.attention_weights = None  # Для сохранения attention weights\n",
    "        self.S = torch.zeros(768, 768)\n",
    "        self.register_buffer('base_mask', torch.tril(torch.ones(c, c)))\n",
    "\n",
    "    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None, \n",
    "                use_cache=False, output_attentions=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Проецирование входных данных\n",
    "        Q = torch.matmul(x, self.Q)  # [batch, seq, hidden]\n",
    "        K = torch.matmul(x, self.K)\n",
    "        V = torch.matmul(x, self.V)\n",
    "        \n",
    "        # Разделение на блоки\n",
    "        num_blocks = seq_len // self.C\n",
    "        remainder = seq_len % self.C\n",
    "        \n",
    "        # Основные блоки\n",
    "        S = torch.zeros(batch_size, self.hidden_dim, self.hidden_dim, \n",
    "                       device=x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            start = i * self.C\n",
    "            end = (i+1) * self.C\n",
    "            \n",
    "            Q_block = Q[:, start:end]  # [batch, C, hidden]\n",
    "            K_block = K[:, start:end]\n",
    "            V_block = V[:, start:end]\n",
    "            K_block_T = K_block.transpose(-1, -2)  # [batch, hidden, C]\n",
    "\n",
    "            # Вычисление внимания\n",
    "            attn_scores = torch.matmul(Q_block, K_block_T)  # [batch, C, C]\n",
    "            attn_scores = attn_scores * self.base_mask\n",
    "            attn_scores = attn_scores / sqrt(self.hidden_dim)\n",
    "            \n",
    "            # Применение масок\n",
    "            if attention_mask is not None:\n",
    "                attn_scores += attention_mask[:, start:end, start:end]\n",
    "            \n",
    "            attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "            \n",
    "            # Обновление состояния\n",
    "            S_update = torch.matmul(K_block_T, V_block)  # [batch, hidden, hidden]\n",
    "            S = S + S_update\n",
    "            \n",
    "            # Вычисление выхода\n",
    "            output = torch.matmul(Q_block, S) + torch.matmul(attn_weights, V_block)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Обработка остатка\n",
    "        if remainder > 0:\n",
    "            start = num_blocks * self.C\n",
    "            Q_remain = Q[:, start:]  # [batch, rem, hidden]\n",
    "            K_remain = K[:, start:]\n",
    "            V_remain = V[:, start:]\n",
    "            \n",
    "            mask = torch.tril(torch.ones(remainder, remainder, device=x.device))\n",
    "            attn_scores = torch.matmul(Q_remain, K_remain.mT) * mask\n",
    "            attn_scores = attn_scores / sqrt(self.hidden_dim)\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                attn_scores += attention_mask[:, start:, start:]\n",
    "            \n",
    "            attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "            output = torch.matmul(Q_remain, S) + torch.matmul(attn_weights, V_remain)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Сборка выходов\n",
    "        O = torch.cat(outputs, dim=1)\n",
    "        return (O,)\n",
    "        \n",
    "\n",
    "\n",
    "class GLA(nn.Module):\n",
    "    def __init__(self, c=5):\n",
    "        super().__init__()\n",
    "        self.gpt2_lmhead = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt2 = self.gpt2_lmhead.transformer\n",
    "        self.lm_head = self.gpt2_lmhead.lm_head\n",
    "        self.config = self.gpt2.config\n",
    "        self.wte = self.gpt2.wte\n",
    "        self.wpe = self.gpt2.wpe\n",
    "        self.drop = self.gpt2.drop\n",
    "        self.ln_f = self.gpt2.ln_f\n",
    "        self.gpt2_layers = []\n",
    "        for i in range(12):\n",
    "            tmp = self.gpt2.h[i]\n",
    "            tmp.attn = GLAAttention(tmp.attn.c_attn.weight[:, :self.config.n_embd], \n",
    "                                     tmp.attn.c_attn.weight[:, self.config.n_embd:2*self.config.n_embd],\n",
    "                                     tmp.attn.c_attn.weight[:, 2*self.config.n_embd:3*self.config.n_embd], c=c)\n",
    "            self.gpt2_layers.append(tmp)\n",
    "\n",
    "    def layers(self):\n",
    "        return self.gpt2_layers, self.config\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_int =X\n",
    "        position_ids = torch.arange(0, X_int.shape[-1], dtype=torch.long)\n",
    "        position_ids = position_ids.unsqueeze(0)\n",
    "        X = self.wte(X_int)\n",
    "        X_p = self.wpe(position_ids)\n",
    "        X+=X_p\n",
    "        X = self.drop(X)\n",
    "        for el in self.gpt2_layers:\n",
    "            X = el.ln_1(X)\n",
    "            X = el.attn(X)\n",
    "            X = el.ln_2(X[0])\n",
    "            X = el.mlp(X)\n",
    "        X = self.ln_f(X)\n",
    "        X = self.lm_head(X)\n",
    "        argmax_indices = torch.argmax(X, dim=2)\n",
    "        return X,argmax_indices  # Возвращаем результат после прохождения через все слои\n",
    "\n",
    "\n",
    "model = GLA(5)\n",
    "logits, output = model(input_ids.int())\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b82db2f7-8aad-41bb-815e-b0dee47c7db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "while i<5:\n",
    "    i+=1\n",
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939f0504-ba23-4cdc-82d0-5bf90f45a195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "  GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GLAAttention()\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )],\n",
       " GPT2Config {\n",
       "   \"_name_or_path\": \"gpt2\",\n",
       "   \"activation_function\": \"gelu_new\",\n",
       "   \"architectures\": [\n",
       "     \"GPT2LMHeadModel\"\n",
       "   ],\n",
       "   \"attn_pdrop\": 0.1,\n",
       "   \"bos_token_id\": 50256,\n",
       "   \"embd_pdrop\": 0.1,\n",
       "   \"eos_token_id\": 50256,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"layer_norm_epsilon\": 1e-05,\n",
       "   \"model_type\": \"gpt2\",\n",
       "   \"n_ctx\": 1024,\n",
       "   \"n_embd\": 768,\n",
       "   \"n_head\": 12,\n",
       "   \"n_inner\": null,\n",
       "   \"n_layer\": 12,\n",
       "   \"n_positions\": 1024,\n",
       "   \"reorder_and_upcast_attn\": false,\n",
       "   \"resid_pdrop\": 0.1,\n",
       "   \"scale_attn_by_inverse_layer_idx\": false,\n",
       "   \"scale_attn_weights\": true,\n",
       "   \"summary_activation\": null,\n",
       "   \"summary_first_dropout\": 0.1,\n",
       "   \"summary_proj_to_labels\": true,\n",
       "   \"summary_type\": \"cls_index\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"task_specific_params\": {\n",
       "     \"text-generation\": {\n",
       "       \"do_sample\": true,\n",
       "       \"max_length\": 50\n",
       "     }\n",
       "   },\n",
       "   \"transformers_version\": \"4.44.2\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50257\n",
       " })"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = model.layers()\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b44e7dd-3220-4c33-9b8f-240c5a6e0b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GLAAttention()\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "437a0c73-73e3-4b15-a3b5-475c49ab17da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2906,  0.3057,  0.0302,  ...,  0.0132, -0.2652, -0.2058],\n",
       "        [-0.3272,  0.2420,  0.2140,  ..., -0.0861,  0.0558,  0.1168],\n",
       "        [-0.2679,  0.1188, -0.2670,  ..., -0.1061,  0.2167,  0.1066],\n",
       "        ...,\n",
       "        [-0.0284,  0.4304, -0.1394,  ..., -0.1750,  0.0154, -0.0614],\n",
       "        [ 0.1730,  0.0967,  0.0262,  ...,  0.1744,  0.3897, -0.2129],\n",
       "        [ 0.0422,  0.1598, -0.2512,  ..., -0.0259,  0.2618,  0.0779]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[1].attn.Q_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ee8d7429-beb5-4f90-9c01-62114f0cad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([1024, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for el in model.parameters():\n",
    "    print(el.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cd846827-d682-4f9f-84f3-6db8f8e8ad03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([1024, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for el in model.parameters():\n",
    "    print(el.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcbc085-5c03-4c63-9250-86f0a23d8ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
